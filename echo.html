<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <title>Echo — P Ananthapadmanabhan Nair</title>
    <link rel="stylesheet" href="styles.css">
    <link rel="stylesheet" href="echo.css">
</head>

<body>
    <nav>
        <div class="container nav-content">
            <div class="nav-title">P Ananthapadmanabhan Nair</div>
            <div class="nav-info"><span>UI/UX Designer & Front-End Developer</span><span>Bengaluru, IN</span></div>
        </div>
    </nav>
    <main class="container project-page">
        <section class="project-hero">
            <h1 class="project-name">Echo</h1>
            <p class="project-meta">Voice Interface Prototype — 2025</p>
            <div class="hero-image"></div>
        </section>

        <section class="project-overview">
            <h2>The Vision</h2>
            <p>Echo explores the invisible interface. It investigates how voice interactions can blend natural dialogue
                with subtle visual feedback to create multi-modal product experiences that feel less like commands and
                more like conversation.</p>
        </section>

        <section class="project-overview">
            <h2>The Challenge</h2>
            <p>Voice interfaces often suffer from a lack of affordance—users don't know what they can say or if they're
                being understood. Echo addresses this by visualizing the "listening state" and providing ambient visual
                cues that mirror the cadence of speech.</p>
        </section>

        <section class="project-details">
            <div class="detail">
                <h3>Role</h3>
                <p>Conversation Design, Voice UX, Prototyping</p>
            </div>
            <div class="detail">
                <h3>Tools</h3>
                <p>Adobe XD, ElevenLabs API, Web Speech API</p>
            </div>
            <div class="detail">
                <h3>Focus</h3>
                <p>Multi-modal Interaction</p>
            </div>
        </section>

        <section class="project-highlights">
            <h2>Technical Deep Dive</h2>
            <p style="color: var(--p-text-muted); font-size: 1.1rem; margin-bottom: 2rem;">
                The prototype uses the Web Speech API for speech-to-text and ElevenLabs for high-quality text-to-speech
                synthesis. A custom Canvas visualization reacts to audio frequency data in real-time.
            </p>
            <ul>
                <li><strong>Ambient Feedback</strong>: Visual particles that react to voice volume and pitch, confirming
                    system attention.</li>
                <li><strong>Contextual Awareness</strong>: The system remembers previous turns in the conversation to
                    maintain context.</li>
                <li><strong>Hybrid Interface</strong>: Seamlessly switching between voice and touch input depending on
                    user preference.</li>
            </ul>
        </section>

        <section class="project-gallery">
            <div class="gallery-item"></div>
            <div class="gallery-item"></div>
        </section>
        <a href="index.html" class="back-link">← Back to Portfolio</a>
    </main>
    <script src="script.js"></script>
</body>

</html>